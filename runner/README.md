# runner

## Architecture

A high level sketch of how the runner is used:

![Architecture](./images/architecture.png)

## Running with Docker

Make sure you have Docker and the [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed with Docker [configured](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-docker) properly and then pull the pre-built image from DockerHub or build the image locally in this directory.

### Pull Docker image

```
docker pull livepeer/ai-runner:latest
```

### Build Docker image

```
docker build -t livepeer/ai-runner:latest .
```

### Models

The runner app within the container references models by their [HuggingFace](https://huggingface.co/) model ID and expects model checkpoints to be stored in a `/models` directory which we can mount with a local `models` directory.

See the `dl-checkpoints.sh` script for how to download model checkpoints to a local `models` directory.

To use `dl-checkpoints.sh` to download model checkpoints:

```
pip install "huggingface_hub[cli]"
./dl-checkpoints.sh
```

### Optimizations

- Set the environment variable `SFAST=true` to enable dynamic compilation with [stable-fast](https://github.com/chengzeyi/stable-fast) to speed up inference for diffusion pipelines (the initial requests will be slower because the model will be dynamically compiled then).

### Run benchmarking script

```
docker run --gpus <GPU_IDs> -v ./models:/models livepeer/ai-runner:latest python bench.py --pipeline <PIPELINE> --model_id <MODEL_ID> --runs <RUNS> --batch_size <BATCH_SIZE>
```

Example command:

```
# Benchmark the text-to-image pipeline with the stabilityai/sd-turbo model over 3 runs using GPU 0
docker run --gpus 0 -v ./models:/models livepeer/ai-runner:latest python bench.py --pipeline text-to-image --model_id stabilityai/sd-turbo --runs 3
```

Example output:

```
----AGGREGATE METRICS----


pipeline load time: 1.473s
pipeline load max GPU memory allocated: 2.421GiB
pipeline load max GPU memory reserved: 2.488GiB
avg inference time: 0.482s
avg inference time per output: 0.482s
avg inference max GPU memory allocated: 3.024s
avg inference max GPU memory reserved: 3.623s
```

For benchmarking script usage information:

```
docker run livepeer/ai-runner:latest python bench.py -h
```

### Run text-to-image container

Run container:

```
docker run --name text-to-image -e PIPELINE=text-to-image -e MODEL_ID=<MODEL_ID> --gpus <GPU_IDS> -p 8000:8000 -v ./models:/models livepeer/ai-runner:latest
```

Query API:

```
curl -X POST -H "Content-Type: application/json" localhost:8000/text-to-image -d '{"prompt":"a mountain lion"}'
```

### Run image-to-image container

Run container:

```
docker run --name image-to-image -e PIPELINE=image-to-image -e MODEL_ID=<MODEL_ID> --gpus <GPU_IDS> -p 8000:8000 -v ./models:/models livepeer/ai-runner:latest
```

Query API:

```
curl -X POST localhost:8000/image-to-image -F prompt="a mountain lion" -F image=@<IMAGE_FILE>
```

### Run image-to-video container

Run container

```
docker run --name image-to-video -e PIPELINE=image-to-video -e MODEL_ID=<MODEL_ID> --gpus <GPU_IDS> -p 8000:8000 -v ./models:/models livepeer/ai-runner:latest
```

Query API:

```
curl -X POST localhost:8000/image-to-video -F image=@<IMAGE_FILE>
```

## Generate OpenAPI spec

The `openapi.json` file contains the OpenAPI spec for the runner.

The file can be re-generated by running:

```
python gen_openapi.py
```

## Deploy on Modal

The runner can be deployed on [Modal](https://modal.com/), a serverless GPU platform.

The `modal_app.py` file contains all the logic for deploying Modal apps for a set of pipelines + model IDs.

Before deploying, make sure to do the following:

- Do a dry-run in a dev [environment](https://modal.com/docs/reference/cli/environment).
- Create an `api-auth-token` [secret](https://modal.com/docs/guide/secrets#secrets) with the `AUTH_TOKEN` environment variable under "Secrets" in the dashboard.
- Run `modal volume create models` to create a network [volume](https://modal.com/docs/guide/volumes#volumes) that will store model weights.
- Run `modal run modal_app.py::download_model --model-id <MODEL_ID>` for each of the model IDs referenced in `modal_app.py`.
  - For gated HuggingFace models (i.e. `stabilityai/stable-video-diffusion-img2vid-xt-1-1`), create an `huggingface` secret under "Secrets" in the dashboard
  with the `HF_TOKEN` environment variable set to your HuggingFace access token.

Then, make sure the apps are deployed:

```
modal deploy modal_app.py
```

The web endpoints for each of the apps will be visible in your dashboard.

## Credits

Based off of [this repo](https://github.com/huggingface/api-inference-community/tree/main/docker_images/diffusers).